day44:
```
    今日进度：
        1.superitcs项目接口转换：30个接口
```
day43:
```
    今日进度：
        1.superitcs项目接口转换：30个接口
```

day42:
```
    今日进度：
        1.superitcs项目接口转换：30个接口
        2.下载景区相关图片
```

day41:
```
    今日进度：
        1.superitcs项目接口转换：30个接口
```

day40:
```
    今日进度：
        1.z-library下载14本故宫相关书籍
        2.搭建superitcs项目基础环境，转换所有接口路径
```

day39:
```
    今日进度：
        1.z-library下载92本故宫相关书籍
        2.将superitcs_v1转为java项目
        3.修改项目技术方案、系统扩展性及安全性文档
```

day38:
```
    今日进度：
        1.z-library下载86本故宫相关书籍
        2.将superitcs_v1转为java项目
        3.编写项目技术方案、系统扩展性及安全性文档
```

day37:
```
    今日进度：
        1.z-library下载98本故宫相关书籍
        2.将趣兜风景区数据保存到mysql中
        3.梳理superitcs项目结构
```

day36:
```
    今日进度：
        1.z-library下载88本故宫相关书籍
        2.uiautomator2抓取趣兜风景区数据
```

day35:
```
    今日进度：
        1.z-library下载73本故宫相关书籍
        2.uiautomator2抓取趣兜风景区数据
```

day34:
```
    今日进度：
        1.z-library下载71本故宫相关书籍
        2.uiautomator2抓取趣兜风北京景区数据
```

day33:
```
    今日进度：
        1.z-library下载78本故宫相关书籍
        2.uiautomator2抓取趣兜风数据及编写景点门票脚本
```

day32:
```
    今日进度：
        1.z-library下载78本故宫相关书籍
        2.uiautomator2抓取数据
```

day31:
```
    今日进度：
        1.z-library下载73本故宫相关书籍
        2.uiautomator2的基本使用
```

day30:
```
    今日进度：
        1.z-library下载76本故宫相关书籍
        2.处理全国景区数据同时与全国行政县区数据进行关联
```

day29:
```
    今日进度：
        1.z-library下载70本故宫相关书籍
        2.处理全国省市县乡村五级行政数据、全国景区数据
```

day28:
```
    今日进度：
        1.z-library下载50本故宫相关书籍
        2.爬取故宫多宝阁数据、数字文物库数据，保存为json文件并保存到mongodb
```

day27:
```
    今日进度：
        1.爬取z-library网站自动化脚本，已完成爬取故宫相关分页数据，共1000条，下载故宫相关书籍80本
        2.爬取故宫名画记网站中pdf文件共10596本
```

day26:
```
    今日进度：
        1.爬取故宫名画记网站数据5000条，并保存为json文件及将之前保存的json文件保存到mongodb中travel_db
        2.编写爬取故宫数字多宝阁自动化脚本，已完成爬取分页数据测试
```

day25:
```
    今日进度：
        1.爬取故宫名画记网站数据15000条，并保存为json文件
        2.z-library下载50本故宫相关书籍
```

day24:
```
    今日进度：
        1.爬取故宫教育、学术、关于相关数据，并保存为json文件
        2.编写爬取故宫名画记网站自动化脚本，已完成爬取分页数据测试
        3.z-library下载10本故宫相关书籍
```

day23:
```
    今日进度：
        1.爬取故宫探索里面剩余藏品、宫廷历史、文物医院、文化专题（列表、详情、论谈（pdf文件）、推荐书目、相关展览），并保存为json文件
```

day22:
```
    今日进度：
        1.调试自动化爬虫，自动爬取故宫所有页面的图片、文本、链接数据
        2.爬取故宫藏品里面法书、铭刻、青铜器（列表、详情、论谈（pdf文件）、推荐书目、相关展览），并保存为json文件
```

day21:
```
    今日进度：
        1.调试自动化爬虫，自动爬取故宫所有页面的图片、文本、链接数据
        2.爬取故宫藏品里面绘画（列表、详情、论谈（pdf文件）、推荐书目、相关展览），并保存为json文件
```

day20:
```
    今日进度：
        1.调试自动化爬虫，自动爬取故宫所有页面的图片、文本、链接数据
        2.爬取故宫藏品里面陶瓷（列表、详情、论谈、推荐书目、相关展览），并保存为json文件
```

day19:
```
    今日进度：
        1.解决pdf爬取不完整问题，爬取故宫建筑、古籍的pdf和图片数据
        2.尝试编写自动化爬虫，自动爬取故宫所有页面的图片、文本数据
```

day18:
```
    今日进度：
        1.爬取故宫建筑数据（列表、详情、论谈、推荐书目），并保存为json文件
        2.爬取故宫古籍数据（列表、详情、论谈、推荐书目、建筑艺术），并保存为json文件
        3.尝试利用cursor配置superitcs基本环境
```

day17:
```
    今日进度：
        1.熟悉superitcs功能模块，尝试利用cursor、通义灵码等ai工具进行将python项目转为java项目
            - 问题：存在转化后无法启动的问题，待解决
                    存在接口路径无法都正确匹配，待解决
```

day16:

```
今日学习内容：
    1.使用pdf2image读取pdf文件，并使用ocr进行文字识别。
        - 获取PDF总页数：
            - pdf2image中的len(convert_from_path(pdf_path, dpi=1))
            - PyPDF2中的len(PyPDF2.PdfReader(file))，耗时时间更短
            - 两者区别：PyPDF2适合读取纯文本类型的pdf，耗时更短，pdf2image适用于读取图片类型的pdf
        -  使用pdf2image读取pdf文件：
            - pdf2image.convert_from_path(pdf_path, dpi=100,startpage=1,lastpage=None)
                控制单次读取页数多少，避免数据过大导致内存溢出

    2.使用playwright爬取故宫数据：
        - 熟悉常用操作和方法：
            - 获取页面元素：
                - 通过 id 获取元素：
                - 通过 class name 获取元素：query_selquerector_all()
                - page.evaluate(() -> {}) 
                - 获取页面所有符合规则的元素：querySelectorAll()
    3.复习了一下Java的项目创建、启动、配置流程

复习题及答案：
    1. Q: pdf2image 和 PyPDF2 在读取 PDF 文件时有何区别？分别适用于哪些场景？
        pdf2image：
            可将 PDF 页面转换为图像（如 PNG、JPEG），适合处理含有图片或扫描内容的 PDF。
            使用方法：convert_from_path(pdf_path, dpi=1)，通过图像 OCR 提取文字。
            场景：OCR 文字识别、处理扫描版 PDF、图像型文档。
        PyPDF2：
            直接读取 PDF 中的文本内容，效率更高，但对图像型 PDF 支持较差。
            使用方法：len(PyPDF2.PdfReader(file)) 获取页数。
            场景：纯文本提取、快速获取 PDF 元信息（如页数）。
    2. Q: Playwright 是什么？它在网页自动化中有哪些优势？
        Playwright 简介：
            是一个支持多浏览器（Chromium、Firefox、WebKit）的自动化测试框架，广泛用于网页爬虫和 UI 自动化测试。
        优势：
            跨浏览器支持： 可控制多种浏览器行为，确保兼容性。
            异步支持良好： 支持 async/await，提升并发性能。
            自动等待机制： 自动等待元素加载完成，减少显式等待逻辑。
            多标签页与上下文管理： 支持多个页面/标签页操作，模拟真实用户行为。
            无头模式： 可以在无界面环境下运行，适合服务器部署。
3. Q: 如何使用 Playwright 定位并操作页面元素？请举例说明常用方法。
    常用定位方式：
        通过 ID 定位： page.locator("#elementId")
        通过类名定位： page.locator(".className")
        通过 CSS 选择器： page.locator("div > span.selector")
        通过 XPath： page.locator("//div[@id='content']")
        操作示例：
            python
            # 点击按钮
            page.click("button#submit")
            # 输入文本
            page.fill("input#username", "test_user")
            # 获取所有匹配元素
            elements = page.query_selector_all("a.link-class")
    4. Q: Playwright 如何等待页面元素加载完成？为什么需要等待机制？
        等待方法：
            page.wait_for_selector(selector)：等待某个元素出现。
            page.wait_for_timeout(5000)：强制等待指定时间（毫秒）。
            page.wait_for_event("load")：等待页面完全加载。
            page.wait_for_function(js_expression)：等待自定义 JS 表达式成立。
        作用：
            避免因元素未加载导致的查找失败。
            模拟用户真实交互流程，提高脚本稳定性。
    5. Q: Playwright 与 Selenium 的主要区别是什么？在实际应用中如何选择？
        特性	         Playwright	                    Selenium
        浏览器支持	    支持 Chromium、Firefox、WebKit	主要支持主流浏览器（Chrome、Firefox、Edge）
        异步支持	     原生支持 async/await	        同步为主，异步需额外封装
        多标签页支持	 原生支持	                    需要切换窗口句柄
        性能	        更快，资源占用更低	            相对较慢
        API 设计	    更现代化，简洁易用	            老旧，部分 API 不够直观
        选择建议：
            Playwright： 适合现代 Web 应用、高并发、异步任务、多浏览器兼容测试。
            Selenium： 适合传统项目、已有 Selenium 生态、企业级自动化测试。
    
```


day15:

```
今日学习内容：
    1. Playwright 的基本使用：
        - 学习了 Playwright 的安装与基本用法，包括页面自动化操作、元素定位、数据提取等。
        - 实践：使用 Playwright 爬取故宫古籍相关数据，包括：
            * 藏书信息
            * 故宫古籍论谈
            * 古籍推荐书目
        - 了解了如何处理页面跳转、等待元素加载、数据保存等常见问题。

复习题：
    1) Playwright 是什么？它主要用于什么场景？
    2) 使用 Playwright 爬取网页数据的基本流程是什么？
    3) 如何在 Playwright 中等待页面元素加载完成？
    4) 简述 Playwright 与 Selenium 的主要区别。

    答案：
    1) Playwright 是一个支持多浏览器的自动化测试框架，常用于网页自动化测试和数据爬取。
    2) 基本流程包括：安装 Playwright → 启动浏览器 → 打开页面 → 定位并操作元素 → 提取数据 → 关闭浏览器。
    3) 可以使用 page.wait_for_selector() 等方法等待元素加载完成。
    4) Playwright 支持更多浏览器（如 Chromium、Firefox、WebKit），并且对异步操作和多标签页支持更好，API 更现代化。
    
```


day14:

```
今日学习内容总结：
    1.Numpy的基本使用：
        - 数组的创建：可以当成不一样的列表，中间没有分隔符
        - 创建数组：
            import numpy as np
            np.array([1,2,3])
        - 数组的运算：
            arr = np.array([1,2,3])
            arr = arr + 1   
        - 索引
        - 切片
        - 生成随机数组：
            np.random.rand(m,n)  生成m行n列的数组
            np.random.randint(m)  生成一个数字，左闭右开
            np.random.randint(a,b,size(m,n))  生成一个数组m行n列，值在a到b之间，左闭右开
    2.Pandas的基本使用：
        - Series：一维数组，类似列表    行或列：一组数据和索引组成
            创建：obj = pd.Series([1,4,2,7,5],index=[1,2,3,4,5])
            切片：
                显示索引：左闭右闭
                隐式索引：左闭右开
            属性：
                name
                shape
                size
                dtype  #里面元素的类型
            方法：
                to_list()
                to_dict()
                isnull()  #判断是否是缺失值（None、Nan）
                _append()
                    # 创建第一个 Series
                s1 = pd.Series([1, 2, 3], index=['a', 'b', 'c'])
                # 创建第二个 Series
                s2 = pd.Series([4, 5, 6], index=['d', 'e', 'f'])
                # 使用 append 方法将 s2 追加到 s1 后面
                result = s1._append(s2, ignore_index=True)  
                # ignore_index=True 表示忽略原索引，生成新索引
                print("拼接后的 Series:")
                print(result)
                drop("a",inplace=true)  #  删除索引为 'a' 的元素,inplace=True 表示直接修改原 Series
            pandas方法读取文件
                pd.read_excel("路径")
                df.head()  #默认读取前五行数据
                df.columns  #输出列名
                df.shape  #输出表中行数和列数
                pd.read_csv("路径")
        - DataFrame：二维数组，类似表格 行和列  二维数据  竖行称之为column，行称之为索引
            创建：
                import pandas as pd
                import numpy as np
                pd.Dataframe(data)
                数组：
                data = np.random.randint(1,9,size=(3,3))
                df = pd.Dataframe(data,index=['a','b'],columns=['m','n'])
                字典和列表组合：
                data = {
                    "Country" : ["China","China","India","India","USA","China","India","Japan"],
                    "Income" : [10000,5001,40000,10000,5000,5001,40000,50000],
                    "Age" : [50,40,25,42,34,40,25,23]
                }

                字典和Series组合：
                data = {
                    "a" :pd.Series([1,2,3]),
                    "b" :pd.Series([4,5,6]),
                    "c" :pd.Series([7,8,9])
                }
            方法：
                unique()    去重
                nunique()   去重后统计
                concat()    拼接
                    从 pandas 2.0 开始，DataFrame.append() 已被 弃用（deprecated），推荐使用 pd.concat()。
                    s = pd.Series(["China",100,50],index=["Country","Income","Age"])
                    df = pd.concat([df,s.to_frame().T],ignore_index=True)
                    s.to_frame().T：把 Series 转换为单行 DataFrame，并转置成横向。
                    pd.concat([...], ignore_index=True)：拼接原 DataFrame 和新行，并重置索引。
            索引：获取特定的值
                loc：显示索引（左闭右闭）
                iloc：隐式索引（左闭右开）
                筛选后赋值
                    筛选：df[df["Age"] % 2 == 0]
                    赋值：
                        new_index = df[df["Age"]% 2 == 0].index
                        df.loc[new_index,"Age"] = 50
            Dataframe读取、写入文件
                df.to_csv("文件路径",index=False)
                pd.read_csv("文件路径")

```

day13:

```
今日学习内容：
    1. Scrapy_splash的基本使用：
        - 介绍：
            Scrapy-splash 是 Scrapy 的一个用于处理 JavaScript 渲染页面的组件。它通过与 Splash 服务交互，实现对需要 JS 渲染的网站内容的抓取。
            Splash 是一个基于 Python 和 Lua 的轻量级浏览器，提供 HTTP API，可以远程渲染网页并返回渲染后的 HTML 源码。其底层基于 Twisted、QT 等模块。
            使用 Scrapy-splash，可以像在浏览器中一样获取页面渲染后的完整内容，解决 Scrapy 无法直接抓取 JS 动态数据的问题。
        - 环境配置：
            - 推荐使用 Splash 的官方 Docker 镜像，便于快速部署和管理。
            - 启动命令示例：`docker run -d --restart = always -p 8050:8050 scrapinghub/splash`
            - 确认本地 8050 端口 Splash 服务可访问。
        - 使用流程：
            1. 创建 Scrapy 项目：`scrapy startproject myproject`
            2. 安装 scrapy-splash：`pip install scrapy-splash`
            3. 在 settings.py 中添加相关配置：
                - 启用 SplashMiddleware
                - 配置 SPLASH_URL（如：http://localhost:8050）
                - 配置去重类和缓存存储为 Splash 兼容版本
            4. 创建爬虫，并在爬虫中使用 SplashRequest 发送需要渲染的请求
            5. 解析渲染后的 response 数据
        - 典型应用场景：
            - 抓取需要登录后才能访问的动态页面
            - 获取通过 JavaScript 加载的内容（如商品价格、评论等）
            - 处理反爬较强的网站时，模拟真实浏览器行为

    2. Scrapy的日志信息、常用配置、其他配置：
        - 日志信息：
            - Scrapy 默认输出详细的日志信息，便于调试和排查问题。
            - 可通过 LOG_LEVEL 设置日志级别（如：DEBUG、INFO、WARNING、ERROR）。
            - 日志可重定向到文件，便于后续分析。
        - 常用配置：
            - USER_AGENT：设置请求头中的 User-Agent，模拟不同浏览器访问。
            - DOWNLOAD_DELAY：设置下载延迟，防止被封禁。
            - CONCURRENT_REQUESTS：设置并发请求数，提高抓取效率。
            - COOKIES_ENABLED：是否启用 Cookie，部分网站需开启。
            - DEFAULT_REQUEST_HEADERS：自定义请求头。
        - 其他配置：
            - RETRY_ENABLED、RETRY_TIMES：请求失败时自动重试次数。
            - AUTOTHROTTLE_ENABLED：自动限速，防止过载目标网站。
            - FEED_EXPORT_ENCODING：导出数据时的编码格式（如 utf-8）。
            - ITEM_PIPELINES：配置数据处理管道。
            - MIDDLEWARES：配置中间件顺序和启用状态。

    3. Scrapyd部署Scrapy项目：
        - Scrapyd 是 Scrapy 官方提供的分布式爬虫部署与管理工具，支持通过 API 远程部署、启动、停止爬虫任务。
        - 部署流程：
            1. 安装 Scrapyd：`pip install scrapyd`
            2. 启动 Scrapyd 服务：`scrapyd`
            3. 使用 scrapyd-deploy 工具将 Scrapy 项目打包并上传到 Scrapyd 服务器
            4. 通过 Scrapyd 的 Web API 或 scrapyd-client 启动、停止、监控爬虫任务
        - 优势：
            - 支持多项目、多爬虫管理
            - 便于团队协作和生产环境部署
    
    4. 解决docker拉取不到镜像的问题：
        - 原因：之前默认使用的阿里镜像源加速服务目前不支持本地docker
        - 解决办法：配置其他的docker镜像源
            sudo tee /etc/docker/daemon.json <<-'EOF'
            {
                "registry-mirrors": [
                    "https://registry.docker-cn.com",
                    "https://dockerproxy.com",
                    "https://docker.mirrors.ustc.edu.cn",
                    "https://docker.nju.edu.cn",
                    "https://hub-mirror.c.163.com",
                    "https://mirror.baidubce.com",
                    "https://docker.m.daocloud.io",
                    "https://docker.imgdb.de",
                    "https://docker-0.unsee.tech",
                    "https://docker.hlmirror.com",
                    "https://cjie.eu.org"
                ]
            }
            EOF

复习题及答案：

    1. 问：Scrapy-splash 主要解决了什么问题？其工作原理是什么？
       答：Scrapy-splash 主要解决 Scrapy 无法抓取 JavaScript 动态渲染内容的问题。其原理是通过 Splash 服务渲染页面，将渲染后的 HTML 返回给 Scrapy，Scrapy 再进行数据提取。

    2. 问：如何在 Scrapy 项目中集成并使用 Scrapy-splash？
       答：步骤包括：安装 scrapy-splash，启动 Splash 服务（推荐用 Docker），在 settings.py 配置 SplashMiddleware、SPLASH_URL、去重类和缓存，爬虫中用 SplashRequest 发送请求。

    3. 问：Scrapy 常用的日志和配置项有哪些？分别有什么作用？
       答：常用日志配置有 LOG_LEVEL（控制日志输出级别）；常用配置有 USER_AGENT（模拟浏览器）、DOWNLOAD_DELAY（限速）、CONCURRENT_REQUESTS（并发数）、COOKIES_ENABLED（Cookie 支持）、RETRY_TIMES（重试次数）等，用于控制爬虫行为和防止被封禁。

    4. 问：Scrapyd 的作用是什么？简述其部署和使用流程。
       答：Scrapyd 用于远程部署和管理 Scrapy 爬虫项目。流程包括：安装并启动 Scrapyd 服务，使用 scrapyd-deploy 上传项目，通过 API 启动/停止爬虫任务，实现分布式和自动化管理。

    5. 问：在抓取需要 JS 渲染的网站时，Scrapy-splash 有哪些优势和注意事项？
       答：优势：可抓取动态内容、模拟真实浏览器渲染、支持脚本注入。注意事项：渲染速度较慢、资源消耗大、需合理设置并发和延迟，避免对目标网站造成压力。

```
day12:

```
今日学习内容总结（优化版）：
    1. Scrapy 管道（pipelines.py）的作用与用法：
        - 主要用于对爬取到的数据进行后续处理，如清洗、验证、存储（如保存到数据库、写入文件等）。
        - 在 pipelines.py 中可以自定义多个处理流程，按优先级依次执行。
    2. CrawlSpider 的使用：
        - CrawlSpider 是 Scrapy 提供的专门用于规则化爬取的爬虫类，适合处理网站的多层级页面抓取。
        - 通过 LinkExtractor 和规则（rules）自动跟进链接，简化复杂网站的爬取逻辑。
    3. Scrapy 中间件的分类与应用：
        - 分类：
            - 下载中间件（Downloader Middleware）：处理请求和响应的中间环节，可用于请求头、代理、cookie 等操作。
            - 爬虫中间件（Spider Middleware）：主要用于处理爬虫输入输出的数据流，如对 item、response 进行处理。
        - 主要作用：
            - 动态更换请求头（User-Agent）、Cookie，防止被封禁。
            - 设置代理 IP，突破访问限制。
            - 对请求和响应进行定制化处理，如自动重试、异常处理等。
        - 实践案例：
            - 实现随机请求头、随机代理。
            - 集成 Selenium 实现动态页面数据抓取。
    4. 分布式爬虫的概念与实现原理：
        - 断点续爬：爬虫在中断后可从上次进度继续，无需重复抓取已完成部分。
        - 请求去重：通过指纹机制避免重复请求同一页面，提高效率。
        - 分布式实现方式：利用 Redis 等消息队列，实现多台机器协同爬取，提升抓取速度和稳定性。
        - 实践案例：分布式爬取笔趣阁小说的分类、列表、详情页数据。

复习题及答案：

    1. 问：Scrapy 的 pipelines 有哪些典型应用场景？如何在项目中启用自定义 pipeline？
    答：典型应用场景包括数据清洗、去重、验证、存储（如写入数据库、导出为文件等）。在 settings.py 中通过 ITEM_PIPELINES 配置启用自定义 pipeline，并设置其优先级。

    2. 问：CrawlSpider 与普通 Spider 有何区别？适合哪些场景？
    答：CrawlSpider 适合需要自动跟进页面链接、进行多层级页面抓取的场景。它通过 rules 和 LinkExtractor 自动提取和跟进链接，而普通 Spider 需手动实现链接提取和请求调度。

    3. 问：Scrapy 的下载中间件和爬虫中间件分别适合处理哪些任务？请举例说明。
    答：下载中间件适合处理请求头、代理、cookie、请求重试等与 HTTP 请求/响应相关的任务。例如：为每个请求设置不同的 User-Agent。爬虫中间件适合处理 item、response 的预处理或过滤等任务，如过滤无效 item。

    4. 问：如何实现 Scrapy 爬虫的断点续爬和请求去重？
    答：可通过启用 Scrapy 的请求去重机制（如 DUPEFILTER_CLASS），并结合持久化调度器（如 scrapy-redis），实现断点续爬和请求去重。

    5. 问：分布式爬虫的优势是什么？Scrapy 如何实现分布式？
    答：分布式爬虫可提升抓取效率、扩展性和容错性。Scrapy 可通过 scrapy-redis 等组件，将请求队列和去重集合存储在 Redis，实现多台机器协同爬取。


day11:

```
今日学习内容总结：
    1. Scrapy 框架应用优化总结：
        - 利用 Scrapy 框架高效爬取网易招聘职位数据，掌握了爬虫项目的基本流程。
        - 实现 Scrapy 模拟登录功能：
            - 通过设置 cookies 参数，模拟用户登录状态，获取需要权限的数据。
            - 使用 FormRequest 发送 POST 请求，适用于如 GitHub 等需要表单登录的网站。

    2. 便民查询网——万年日历数据爬取与解析：
        - 巩固了 Python 基础语法及常用数据处理技巧：
            - resp.raise_for_status()：自动检测响应状态码，遇到 4xx/5xx 错误时抛出 HTTPError 异常，便于异常处理。
            - 字符串分割与类型转换：如 year, month, _ = map(int, date_str.split('-'))，快速提取年月信息。
            - any(...) 用法：如 any(fest in name for fest in lunar_festivals)，判断某元素是否存在于序列中。
            - 字典推导式（Dictionary Comprehension）：如 day_info['festivals'] = {k: v for k, v in festivals.items() if v}，高效筛选非空节日信息。

    复习题及答案：

    1. 问：如何在 Scrapy 中实现模拟登录？请简述常用方法及其原理。
       答：在 Scrapy 中实现模拟登录常用的方法有两种：
           （1）通过设置 cookies 参数，将已登录用户的 cookies 添加到请求中，模拟登录状态，从而获取需要权限的数据。
           （2）使用 scrapy.FormRequest 发送 POST 请求，提交登录表单数据（如用户名和密码），适用于需要表单登录的网站。原理是模拟浏览器提交登录表单，服务器验证通过后返回登录后的会话信息，后续请求即可保持登录状态。

    2. 问：resp.raise_for_status() 的作用是什么？遇到什么情况会抛出异常？
       答：resp.raise_for_status() 的作用是检测 HTTP 响应的状态码。如果响应状态码为 4xx 或 5xx（即客户端或服务器错误），则会抛出 HTTPError 异常，便于程序进行异常处理。

    3. 问：写出将 "2024-06-01" 拆分为 year、month、day 三个整数的 Python 代码。
       答：代码如下：
           year, month, day = map(int, "2024-06-01".split('-'))

    4. 问：any() 函数的典型应用场景是什么？请举例说明。
       答：any() 函数常用于判断可迭代对象中是否至少有一个元素为 True。典型应用场景如：判断某个元素是否存在于序列中。例如：
           lunar_festivals = ['春节', '端午节', '中秋节']
           name = '今天是中秋节'
           result = any(fest in name for fest in lunar_festivals)  # 如果 name 中包含任意一个节日，result 为 True

    5. 问：如何用字典推导式筛选出字典中值不为空的键值对？请写出示例代码。
       答：可以使用字典推导式筛选出值不为空的键值对。示例代码如下：
           filtered_dict = {k: v for k, v in original_dict.items() if v}
```

day10:

```
今日学习内容总结：
    1.Scrapy初步使用：
        - Scrapy中各组件的功能：
            Scrapy Engine（引擎）：Scrapy框架的核心部分。负责在Spider和ItemPipeline、Downloader、Scheduler中间通信、传递数据等。
            Spider（爬虫）：发送需要爬取的链接给引擎，最后引擎把其他模块请求回来的数据再发送给爬虫，爬虫就去解析想要的数据。这个部分是我们开发者自己写的，因为要爬取哪些链接，页面中的哪些数据是我们需要的，都是由程序员自己决定。
            Scheduler（调度器）：负责接收引擎发送过来的请求，并按照一定的方式进行排列和整理，负责调度请求的顺序等。
            Downloader（下载器）：负责接收引擎传过来的下载请求，然后去网络上下载对应的数据再交还给引擎。
            Item Pipeline（管道）：负责将Spider（爬虫）传递过来的数据进行保存。具体保存在哪里，应该看开发者自己的需求。
            Downloader Middlewares（下载中间件）：可以扩展下载器和引擎之间通信功能的中间件。
            Spider Middlewares（Spider中间件）：可以扩展引擎和爬虫之间通信功能的中间件。
        - Scrapy快速入门：
            - 使用srcapy startproject myproject 创建基础结构
            - cd myproject、 scrapy genspider myspider spider_url 创建爬虫
                - settings.py配置基础信息
                - myspider.py中实现功能代码
            - srcapy crawl myspider 启动项目
            - 实现了scrapy爬取豆瓣top250、传智播客教师列表
    2.WordPress平台上完成了站点内容的初步配置：
        - 配置项目中心: 使用文章中的分类和菜单的层级结构实现项目的层级结构（如项目列表、具体项目），便于分层分级展示。
        - 配置下载中心:
        - 安装并启用“下载管理”插件（Wordpress Download Manager）。
        - 新建“下载”自定义文章类型，上传文件并填写下载说明。
        - 配置下载权限，启用搜索功能，方便用户查找文件。
        - 在页面中插入索框，提升用户体验。

```

day09:

```
今日学习内容总结：

    1. 学习并实践了WordPress功能扩展，包括SEO优化、安全加固、缓存插件的安装与基础配置。
    2. 用思维导图工具梳理了WordPress常用工具及其使用方法，提升了整体理解。
    3. 掌握了禅道中需求与任务的关联操作，学习了父子任务的创建与管理流程。
    4. WordPress平台上完成了站点内容的初步配置：
        - 规划了站点结构（首页、公司介绍、项目中心、下载中心）
        - 配置并美化了首页
        - 完成了公司介绍页面的内容搭建

今日复习题及答案：

    1. 问：WordPress中常用的SEO插件有哪些？基本配置步骤是什么？
    答：常用SEO插件有Yoast SEO、All in One SEO Pack等。基本配置包括：设置站点标题与描述、生成XML网站地图、优化页面关键词、配置社交媒体信息等。
    2. 问：如何通过插件提升WordPress站点的安全性？
    答：可以安装如Wordfence Security、iThemes Security等插件，进行防火墙设置、登录保护、恶意代码扫描、定期备份等操作。
    3. 问：禅道中如何实现需求与任务的关联及父子任务的管理？
    答：在禅道中可通过“创建子任务”功能，将一个需求拆分为多个子任务，并在任务详情页进行关联和层级管理，便于项目进度追踪。
    4. 问：WordPress站点结构规划时应注意哪些要点？
    答：应根据实际需求合理划分页面（如首页、公司介绍、项目中心、下载中心），确保导航清晰、内容分区明确，便于用户访问和后期维护。
```



day08:

```
今日学习内容总结：
    1.aiohttp异步爬取图片、小说
    2.wordpress工具的基本使用：文章排版、主题搜索和自定义、菜单显示和配置
```



day07:    

```
今日学习内容优化总结：

    1. 防盗链机制与处理
        - 学习了防盗链的原理和常见实现方式
        - 实践：通过修改请求头中的Referer字段绕过防盗链限制

    2. 代理IP的使用
        - 掌握了代理IP的基本概念和使用方法
        - 实践：使用代理IP池进行数据爬取，避免IP被封禁

    3. 网易云音乐评论爬取
        - 分析了网易云音乐API的请求参数和加密方式
        - 实践：成功抓取指定歌曲的评论信息
        
    4. 线程池爬虫实战
        - 学习了线程池的基本概念和使用方法
        - 实践：使用线程池并发爬取新发地菜价信息，提高爬取效率

今日复习题及答案：

    1. 什么是防盗链？如何绕过防盗链限制？
    答：防盗链是一种防止其他网站直接引用自己网站资源的技术。可以通过修改请求头中的Referer字段来模拟正常访问，例如：

    ```python
    headers = {
    'Referer': 'https://www.example.com',
    'User-Agent': 'Mozilla/5.0 ...'
    }
    ```

    2. 代理IP在爬虫中的作用是什么？如何实现代理IP池？
    答：代理IP可以隐藏真实IP，避免被封禁。实现代理IP池的基本步骤：
        1）获取代理IP列表（可购买或爬取免费代理）
        2）验证代理IP可用性
        3）维护代理IP池，定期更新
        4）随机选择代理IP使用
    
    3. 网易云音乐评论爬取需要注意哪些问题？如何处理加密参数？
    答：需要注意：
        1）分析API请求参数，特别是加密参数
        2）模拟加密算法生成必要参数
        3）处理反爬机制（如频率限制）
        4）注意评论数据的编码和格式
    4. 线程池在爬虫中的应用场景是什么？如何正确使用线程池？
    答：线程池适用于I/O密集型任务，如网页爬取。使用步骤：
        1）创建线程池：`ThreadPoolExecutor(max_workers=5)`
        2）提交任务：`executor.submit(task, args)`
        3）获取结果：`future.result()`
        4）注意控制并发数量，避免对目标服务器造成压力
```



day06:

```
今日学习内容优化总结：

    1. 正则表达式（re）基础  
        - 学习了正则表达式的基本语法和常用方法  
        - 实践：爬取豆瓣Top250电影排行，并将数据输出为CSV文件

    2. BeautifulSoup（bs4）解析入门  
        - 掌握了BeautifulSoup的基本用法  
        - 实践：抓取优乐图库的图片并实现批量下载

    3. XPath解析基础与实战  
        - 学习了XPath的基本语法和常用表达式  
        - 实践：抓取猪八戒网的相关信息

今日复习题及答案：

    1. 简述正则表达式在爬虫中的作用，并举例说明如何用re模块提取网页中的指定内容。
    答：正则表达式用于匹配和提取网页中的特定文本内容。例如，使用re.findall(r'<title>(.*?)</title>', html)可以提取网页标题。

    2. BeautifulSoup和XPath各自的优缺点是什么？分别适合哪些场景？
    答：BeautifulSoup语法简单，适合处理结构不规范的HTML，学习成本低；XPath功能强大，适合结构清晰、标签层级分明的页面，提取效率高。

    3. 如何将爬取到的数据保存为CSV文件？请简述基本步骤。
    答：1）导入csv模块；2）打开文件并创建writer对象；3）写入表头和数据行；4）关闭文件。例如：  
        import csv
        with open('data.csv', 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['标题', '评分'])
        writer.writerows(data_list)
```



day05:

```
今日学习内容优化总结：

    1. 掌握了Python基础爬虫的原理与实现，包括HTTP请求方法（GET、POST）、常用库（requests、BeautifulSoup、lxml）的使用，能够实现网页数据的抓取与解析。
    2. 学习了Python与MySQL数据库的连接方法，熟悉了pymysql库的基本用法，能够进行MySQL数据库的增、删、改、查等基础操作。

今日复习题及答案：

    1. 简述Python实现网页爬虫的基本流程，并说明GET和POST请求的区别。
    答：基本流程包括：发送HTTP请求（GET/POST）获取网页内容，解析网页数据（如用BeautifulSoup、lxml），提取所需信息。GET用于获取资源，参数在URL中明文传递；POST用于提交数据，参数在请求体中，适合敏感或大量数据。

    2. requests、BeautifulSoup和lxml各自的作用是什么？
    答：requests用于发送HTTP请求获取网页内容；BeautifulSoup和lxml用于解析和提取HTML/XML中的数据。

    3. Python如何连接MySQL数据库？请简述基本步骤。
    答：常用pymysql库，基本步骤：1）导入pymysql模块；2）建立连接（pymysql.connect）；3）创建游标（cursor）；4）执行SQL语句；5）获取结果；6）关闭游标和连接。

    4. 请写出一条MySQL的基础增、删、改、查（CRUD）SQL语句示例。
    答：增：INSERT INTO table_name (col1, col2) VALUES (val1, val2);
        删：DELETE FROM table_name WHERE condition;
        改：UPDATE table_name SET col1=val1 WHERE condition;
        查：SELECT * FROM table_name WHERE condition;
```



day04:

```
今日学习内容总结：

    1. 理解了可迭代对象（实现`__iter__()`方法）、迭代器对象（实现`__next__()`方法）、生成器（包含`yield`关键字）的概念及其关系和基本用法：生成器是特殊的迭代器，迭代器是特殊的可迭代对象。
    2. 掌握了线程、进程与协程的区别及适用场景，了解了它们在并发编程中的作用以及基本用法。
    3. 学习了正则表达式的基本语法及其在文本处理中的应用。
    4. 熟悉了os（操作系统交互）、sys（python解释器信息）、time（时间戳、格式化、时间元组）、logging（日志记录、问题分析和定位）等常用模块的功能和基本用法。
    5. 复习了Linux系统下的常用命令。


今日复习题及答案：

    1. 可迭代对象、迭代器对象和生成器的概念及关系是什么？
    答：可迭代对象是实现了`__iter__()`方法的对象，可以被for循环遍历，如list、dict等。迭代器对象实现了`__iter__()`和`__next__()`方法，可以逐个取出元素。
    生成器是包含`yield`关键字的特殊迭代器，能动态生成数据。三者关系：生成器是特殊的迭代器，迭代器是特殊的可迭代对象。

    2. 如何判断一个对象是可迭代对象？如何判断一个对象是迭代器？
    答：可用`collections.abc.Iterable`判断可迭代对象，如`isinstance(obj, Iterable)`；用`collections.abc.Iterator`判断迭代器，如`isinstance(obj, Iterator)`。

    3. 生成器与普通迭代器有何区别？请举例说明生成器的基本用法。
    答：生成器通过函数和`yield`语句创建，能动态生成数据，节省内存；普通迭代器通常通过类实现。示例：

    ```
    def gen():
    	for i in range(3):
    		yield i
    g = gen()
    print(next(g))  # 输出0
    ```

    4. 线程、进程和协程分别适用于哪些场景？有何区别？
    答：线程适合I/O密集型任务，进程适合CPU密集型任务，协程适合高并发、轻量级任务。区别：进程间独立，线程共享内存，协程由用户调度，切换开销小。

    5. 在Python中，如何实现多线程和多进程？协程的基本用法是什么？
    答：多线程可用`threading`模块，多进程用`multiprocessing`模块。协程可用`asyncio`模块，定义`async def`函数并用`await`调用异步操作。
    6. 正则表达式的常用语法有哪些？请写出一个匹配邮箱地址的正则表达式示例。
    答：常用语法有：.（任意字符）、*（重复）、+（至少一次）、?（可有可无）、[]（字符集）、^（开头）、$（结尾）、\d（数字）、\w（字母数字下划线）。邮箱正则示例：`[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+`

    7. os、sys、time、logging模块分别有哪些常用功能？请各举一例说明其应用场景。
    答：os用于操作系统交互，如`os.listdir()`列出目录文件；sys用于获取解释器信息，如`sys.argv`获取命令行参数；time用于时间处理，如`time.time()`获取当前时间戳；logging用于日志记录，如`logging.info("信息")`。

    8. Linux系统下常用的命令有哪些？请列举至少五个并简要说明其作用。
    答：ls（列出目录内容）、cd（切换目录）、cp（复制文件）、mv（移动/重命名文件）、rm（删除文件）、cat（查看文件内容）、grep（文本搜索）、chmod（修改权限）。
```



day03:

```
复习python语法基础：
    1. 熟悉了闭包、装饰器及其语法糖的使用。
    2. 理解了被装饰函数的执行流程，以及多个装饰器叠加时的调用顺序。
    3. 学习了面向对象编程，掌握了类与对象的关系、实例方法、实例属性、构造函数和析构函数的用法。
    4. 掌握了封装、继承（包括单继承、方法重写和多继承）以及多态的实现方式。
    5. 了解了静态方法、类方法、单例模式和常用魔法方法的应用场景。
    6. 熟悉了文件的基本操作，包括文件的读写、访问、定位、with open()语句的使用、编码格式处理以及目录的获取和操作。

面试题及答案：

    1. 闭包是什么？请举例说明其应用场景。
    答：闭包是指在一个函数内部定义的函数可以引用外部函数的变量，即使外部函数已经执行完毕。常用于工厂函数、延迟计算等场景。例如：
    ```
    def outer(x):
    def inner(y):
    return x + y
    return inner
    add5 = outer(5)
    print(add5(3))  # 输出8
    ```
    2. 什么是装饰器？多个装饰器叠加时，函数的执行顺序是怎样的？
    答：装饰器本质上是一个函数，用于在不修改原函数代码的情况下，动态地增加功能。多个装饰器叠加时，最靠近被装饰函数的装饰器最后执行。例如：

    ```
    @A
    @B
    def func():
        pass
    # 实际执行顺序为A(B(func))
    ```
    3. 面向对象编程中，类与对象的关系是什么？实例方法、实例属性、构造函数和析构函数分别有什么作用？
    答：类是对象的模板，对象是类的实例。实例方法用于操作对象属性，实例属性用于存储对象的状态，构造函数`__init__`用于初始化对象，析构函数`__del__`用于对象销毁时的清理工作。
    4. 如何实现封装、继承和多态？请简述单继承、方法重写和多继承的区别。
    答：封装通过属性和方法隐藏内部实现，继承通过子类复用父类代码，多态通过统一接口调用不同子类实现。单继承指一个子类只继承一个父类，多继承指一个子类继承多个父类，方法重写是子类对父类方法的重新实现。
    5. 静态方法、类方法、单例模式和常用魔法方法分别适用于哪些场景？
    答：静态方法适用于与类无关的功能，类方法用于操作类属性或工厂方法，单例模式用于保证类只有一个实例，常用魔法方法如`__str__`、`__repr__`、`__len__`等用于自定义对象的行为。
    6. Python中文件操作的基本流程是什么？如何安全地读写文件并处理编码问题？
    答：文件操作流程包括打开文件、读写内容、关闭文件。推荐使用`with open()`语句自动管理文件关闭。处理编码时可指定`encoding`参数，如`open('file.txt', 'r', encoding='utf-8')`。
    常用操作包括`read()`、`write()`、`seek()`等，目录操作可用`os`模块。
```



day02: 

```
复习python语法基础：
    while与for循环、break（结束本轮循环）与continue（跳过本次循环）关键字
    字符串编码与字符串常见操作
    列表的定义与增删改查
    元组（不可变对象）、元组的定义与字典的增删改查
    集合的格式及其使用
    类型转换：int、float、str、eval（用来执行一个字符串表达式，并返回表达式的值）、tuple、list、chr()
    赋值、深浅拷贝、可变（列表、集合、字典）与不可变对象（数值、字符串、元组）
    函数、return返回值与形参实参
    函数的各类参数（可变参数*args）与函数嵌套
    作用域、匿名函数与匿名函数的参数
    lambda结合if判断、内置函数与拆包
    异常模块与包
```



day01: 

```
软件准备（飞书、cursor）
复习python语法基础：
    debug、注释与输出函数
    变量与标示符
    数值类型（int、float）、字符串类型（str）与格式化输出（%d、%s、%f）
    算数与赋值运算符(+、-、*、/、%)、输入函数与转义字符（\t、\n）
    逻辑判断（if）和逻辑运算符（and、or、not）
    条件判断（if-else、if-elif与if嵌套）
```

